# Consistency Enhancement Roadmap
## From AI-Driven to Geometry-Driven Architecture

**Status**: Analysis Complete | Phased Implementation Plan
**Goal**: Achieve deterministic 99.9%+ consistency through procedural geometry

---

## Executive Summary

Your architectural proposal represents a paradigm shift from **prompt-engineered AI generation** to **deterministic geometry generation with AI stylization**. This document outlines a pragmatic 3-phase migration path that preserves current functionality while incrementally moving toward full geometric determinism.

**Current Achievement**: 98%+ consistency via DNA system (prompt engineering)
**Target Achievement**: 99.9%+ consistency via procedural geometry (deterministic)

---

## Current Architecture Analysis

### What Already Works Well

The existing DNA system (src/services/enhancedDNAGenerator.js) already produces **structured JSON specifications** remarkably similar to your proposal:

```javascript
{
  projectID: "DN15-8BQ-TERRACED",
  seed: 63485,
  dimensions: {
    length: 15,
    width: 10,
    totalHeight: 6.5,
    floorCount: 2,
    wallThickness: "0.3m exterior, 0.15m interior"
  },
  materials: {
    exterior: { primary: "Red clay brick", color: "#8B4513" },
    roof: { type: "gable", material: "Clay tiles", color: "#654321", pitch: "35¬∞" },
    windows: { type: "Casement", frame: "UPVC", color: "#FFFFFF", standardSize: "1.5m √ó 1.2m" }
  },
  floorPlans: {
    ground: {
      rooms: [
        { name: "Living Room", dimensions: "5.5m √ó 4.0m", area: "22m¬≤", position: "Front left" }
      ]
    }
  },
  elevations: {
    north: { description: "FRONT FACADE", features: ["Main entrance centered", "4 windows ground + 4 upper"] }
  }
}
```

**This is 80% of your proposed design.json already implemented!**

### Current Pipeline

```
User Input ‚Üí Qwen 2.5 72B ‚Üí Master DNA JSON ‚Üí 13 Unique Prompts ‚Üí FLUX.1-dev ‚Üí 13 Images
```

**Bottleneck**: The transition from DNA JSON ‚Üí Prompts ‚Üí AI loses geometric precision

### Identified Issues

1. **Axonometric ‚âà Perspective Bug**: Both views generated but UI may display same image (state management)
2. **No Geometric Validation**: DNA dimensions not checked for topology, min room sizes, door clearances
3. **No Persistent State**: Design JSON regenerated each time, no project history
4. **AI Controls Everything**: Even with perfect prompts, FLUX can deviate (70-98% consistency ceiling)

---

## Proposed Architecture

```
User Input ‚Üí design.json ‚Üí Geometry Engine ‚Üí Renders (2D/3D) ‚Üí [Optional] AI Stylization
```

**Key Innovation**: Geometry is source of truth, AI only adds photorealistic textures

---

## Phase 1: Quick Wins (1-2 Days)
### Immediate improvements without major refactoring

### 1.1 Fix Axonometric/Perspective Duplicate Image Bug

**Problem**: User reports both views showing same image

**Root Cause Analysis** (from code review):
- Prompts are correctly differentiated (axonometric: 45¬∞ isometric, perspective: 2-point eye-level)
- Seed offsets different (axonometric_3d: +0, perspective_3d: +1)
- Issue likely in ArchitectAIEnhanced.js state management

**Fix**: Add unique IDs and separate state variables

```javascript
// BEFORE (lines 227-229)
const axonometricUrl = result?.visualizations?.views?.axonometric?.images?.[0];
const perspectiveUrl = result?.visualizations?.views?.perspective?.images?.[0];

// AFTER - Add defensive checks and unique keys
const axonometricUrl = result?.visualizations?.views?.axonometric?.images?.[0];
const perspectiveUrl = result?.visualizations?.views?.perspective?.images?.[0];

// Add diagnostic logging
if (axonometricUrl && perspectiveUrl) {
  if (axonometricUrl === perspectiveUrl) {
    console.error('üêõ BUG: Axonometric and Perspective are SAME URL:', axonometricUrl);
    console.error('   This means image generation failed to produce unique views');
  } else {
    console.log('‚úÖ Axonometric and Perspective are DIFFERENT URLs');
  }
}

// In image display, add unique keys
<img
  key={`axon-${result.timestamp}-${axonometricUrl}`}
  src={axonometricUrl}
  alt="Axonometric View"
/>
<img
  key={`persp-${result.timestamp}-${perspectiveUrl}`}
  src={perspectiveUrl}
  alt="Perspective View"
/>
```

**Implementation**:
- File: `src/ArchitectAIEnhanced.js` lines 227-229, 2000-2100
- Add logging to detect if same URL returned
- Add unique React keys to force re-render
- Test: Generate design, check browser console for "SAME URL" error

### 1.2 Persist Design State (design.json)

**Create single source of truth** that persists across sessions

```javascript
// src/services/designStateManager.js (NEW FILE)
class DesignStateManager {
  constructor() {
    this.currentDesign = null;
  }

  saveDesign(masterDNA, projectContext, location, siteMetrics) {
    const design = {
      design_id: `proj_${Date.now()}_${masterDNA.projectID}`,
      version: "1.0",
      timestamp: new Date().toISOString(),

      // Site context
      site: {
        address: location?.address,
        lat: location?.coordinates?.lat,
        lon: location?.coordinates?.lng,
        north: siteMetrics?.orientationDeg || 0,
        areaM2: siteMetrics?.areaM2,
        polygon: siteMetrics?.polygon
      },

      // DNA = design tokens
      dna: {
        seed: masterDNA.seed,
        style: masterDNA.materials?.exterior?.primary,
        climate: projectContext.climate?.type,
        module_mm: 300, // TODO: extract from DNA
        wwr: this.calculateWWR(masterDNA),
        roof: {
          type: masterDNA.materials?.roof?.type,
          pitch: masterDNA.materials?.roof?.pitch,
          material: masterDNA.materials?.roof?.material,
          color: masterDNA.materials?.roof?.color
        },
        materials: masterDNA.materials
      },

      // Geometric specification
      dimensions: {
        length: masterDNA.dimensions?.length,
        width: masterDNA.dimensions?.width,
        height: masterDNA.dimensions?.totalHeight,
        floorCount: masterDNA.dimensions?.floorCount
      },

      // Room specifications (prepare for geometry conversion)
      levels: this.extractLevels(masterDNA),
      rooms: this.extractRooms(masterDNA),
      doors: this.extractDoors(masterDNA),
      windows: this.extractWindows(masterDNA),

      // View configurations
      cameras: {
        axon: { type: "ortho", az: 45, el: 30, dist: 22, fov: 20 },
        persp: { type: "persp", az: 60, el: 20, dist: 26, fov: 60 },
        interior_main: { type: "persp", target: "living", fov: 70 }
      },

      // Generated outputs
      visualizations: {
        floorPlans: [],
        elevations: [],
        sections: [],
        exterior3D: [],
        interior3D: []
      }
    };

    // Save to localStorage
    localStorage.setItem(`design_${design.design_id}`, JSON.stringify(design));
    this.currentDesign = design;

    console.log('üíæ Design saved:', design.design_id);
    return design;
  }

  loadDesign(designId) {
    const saved = localStorage.getItem(`design_${designId}`);
    if (saved) {
      this.currentDesign = JSON.parse(saved);
      return this.currentDesign;
    }
    return null;
  }

  calculateWWR(masterDNA) {
    // Calculate window-to-wall ratio from DNA
    // TODO: implement based on window counts and facade areas
    return 0.25; // default
  }

  extractLevels(masterDNA) {
    return [
      { z: 0, height_mm: parseFloat(masterDNA.dimensions?.groundFloorHeight) * 1000 },
      { z: parseFloat(masterDNA.dimensions?.groundFloorHeight) * 1000,
        height_mm: parseFloat(masterDNA.dimensions?.upperFloorHeight) * 1000 }
    ];
  }

  extractRooms(masterDNA) {
    const rooms = [];

    // Ground floor rooms
    masterDNA.floorPlans?.ground?.rooms?.forEach((room, idx) => {
      const [length, width] = this.parseDimensions(room.dimensions);
      rooms.push({
        id: `rm_ground_${idx}`,
        name: room.name,
        level: 0,
        dimensions: room.dimensions,
        area: room.area,
        // TODO: convert position to polygon coordinates
        poly: this.positionToPoly(room.position, length, width)
      });
    });

    // Upper floor rooms
    masterDNA.floorPlans?.upper?.rooms?.forEach((room, idx) => {
      const [length, width] = this.parseDimensions(room.dimensions);
      rooms.push({
        id: `rm_upper_${idx}`,
        name: room.name,
        level: 1,
        dimensions: room.dimensions,
        area: room.area,
        poly: this.positionToPoly(room.position, length, width)
      });
    });

    return rooms;
  }

  parseDimensions(dimStr) {
    // Parse "5.5m √ó 4.0m" ‚Üí [5.5, 4.0]
    const match = dimStr.match(/(\d+\.?\d*)m?\s*√ó\s*(\d+\.?\d*)m?/);
    if (match) {
      return [parseFloat(match[1]), parseFloat(match[2])];
    }
    return [5, 4]; // default
  }

  positionToPoly(position, length, width) {
    // TODO: Convert textual position ("Front left") to actual coordinates
    // This requires spatial reasoning - Phase 2 enhancement
    return [[0, 0], [length * 1000, 0], [length * 1000, width * 1000], [0, width * 1000]];
  }

  extractDoors(masterDNA) {
    // TODO: Extract door specifications from DNA
    return [];
  }

  extractWindows(masterDNA) {
    // TODO: Extract window specifications from DNA
    return [];
  }
}

export default new DesignStateManager();
```

**Integration**:
```javascript
// In ArchitectAIEnhanced.js, after DNA generation
import designStateManager from './services/designStateManager';

// After masterDNA is generated (line ~1400)
const designState = designStateManager.saveDesign(
  masterDNA,
  projectContext,
  locationData,
  siteMetrics
);
console.log('üíæ Design state persisted:', designState.design_id);
```

### 1.3 Add Pre-Generation Validation Layer

**Validate DNA before any image generation**

```javascript
// src/services/designValidator.js (NEW FILE)
class DesignValidator {
  validate(masterDNA) {
    const errors = [];
    const warnings = [];

    // 1. TOPOLOGY CHECKS
    const topoErrors = this.validateTopology(masterDNA);
    errors.push(...topoErrors);

    // 2. DIMENSION CHECKS
    const dimErrors = this.validateDimensions(masterDNA);
    errors.push(...dimErrors);

    // 3. MATERIAL COMPATIBILITY
    const matWarnings = this.validateMaterials(masterDNA);
    warnings.push(...matWarnings);

    // 4. ROOM ADJACENCY (if spatial data available)
    const adjWarnings = this.validateAdjacency(masterDNA);
    warnings.push(...adjWarnings);

    return {
      valid: errors.length === 0,
      errors,
      warnings,
      autoFixes: this.generateAutoFixes(errors, warnings, masterDNA)
    };
  }

  validateTopology(dna) {
    const errors = [];

    // Check total floor area vs building footprint
    const footprint = dna.dimensions?.length * dna.dimensions?.width;
    const groundArea = this.sumRoomAreas(dna.floorPlans?.ground?.rooms);

    if (groundArea > footprint * 1.1) {
      errors.push({
        type: 'TOPOLOGY',
        severity: 'ERROR',
        message: `Ground floor rooms (${groundArea.toFixed(1)}m¬≤) exceed building footprint (${footprint.toFixed(1)}m¬≤)`,
        fix: 'Reduce room sizes or increase building dimensions'
      });
    }

    // Check for minimum circulation space
    const circulationMin = footprint * 0.15; // 15% for hallways/stairs
    if (groundArea > footprint * 0.85) {
      errors.push({
        type: 'TOPOLOGY',
        severity: 'WARNING',
        message: `Insufficient circulation space (${((footprint - groundArea) / footprint * 100).toFixed(1)}% vs 15% minimum)`,
        fix: 'Reduce room sizes to allow for hallways and stairs'
      });
    }

    return errors;
  }

  validateDimensions(dna) {
    const errors = [];

    // Check minimum room dimensions
    const rooms = [
      ...(dna.floorPlans?.ground?.rooms || []),
      ...(dna.floorPlans?.upper?.rooms || [])
    ];

    rooms.forEach(room => {
      const [length, width] = this.parseDimensions(room.dimensions);

      // Minimum habitable room size: 2.4m x 2.4m
      if (length < 2.4 || width < 2.4) {
        errors.push({
          type: 'DIMENSIONS',
          severity: 'ERROR',
          message: `Room "${room.name}" too small (${room.dimensions}). Minimum 2.4m √ó 2.4m`,
          fix: `Increase ${room.name} to at least 2.4m √ó 2.4m`
        });
      }

      // Bathroom minimum: 1.8m x 1.5m
      if (room.name.toLowerCase().includes('bathroom') && (length < 1.8 || width < 1.5)) {
        errors.push({
          type: 'DIMENSIONS',
          severity: 'ERROR',
          message: `Bathroom too small (${room.dimensions}). Minimum 1.8m √ó 1.5m`,
          fix: 'Increase bathroom size'
        });
      }
    });

    // Check floor height minimums
    const groundHeight = parseFloat(dna.dimensions?.groundFloorHeight);
    const upperHeight = parseFloat(dna.dimensions?.upperFloorHeight);

    if (groundHeight < 2.4) {
      errors.push({
        type: 'DIMENSIONS',
        severity: 'ERROR',
        message: `Ground floor height ${groundHeight}m < 2.4m minimum`,
        fix: 'Increase ground floor height to 2.4m minimum'
      });
    }

    if (upperHeight < 2.3) {
      errors.push({
        type: 'DIMENSIONS',
        severity: 'ERROR',
        message: `Upper floor height ${upperHeight}m < 2.3m minimum`,
        fix: 'Increase upper floor height to 2.3m minimum'
      });
    }

    return errors;
  }

  validateMaterials(dna) {
    const warnings = [];

    // Check material compatibility
    const exterior = dna.materials?.exterior?.primary;
    const roof = dna.materials?.roof?.material;

    // Warn about unusual combinations
    const incompatible = {
      'Glass': ['Clay tiles', 'Slate'],
      'Timber': ['Concrete tiles']
    };

    if (incompatible[exterior]?.includes(roof)) {
      warnings.push({
        type: 'MATERIALS',
        severity: 'WARNING',
        message: `Unusual combination: ${exterior} walls with ${roof} roof`,
        fix: 'Consider alternative material pairing'
      });
    }

    return warnings;
  }

  validateAdjacency(dna) {
    const warnings = [];

    // Check logical room adjacency
    const groundRooms = (dna.floorPlans?.ground?.rooms || []).map(r => r.name.toLowerCase());

    // Bathrooms shouldn't open directly to kitchens
    const hasBathroom = groundRooms.some(r => r.includes('bathroom'));
    const hasKitchen = groundRooms.some(r => r.includes('kitchen'));

    if (hasBathroom && hasKitchen) {
      warnings.push({
        type: 'ADJACENCY',
        severity: 'WARNING',
        message: 'Bathroom and kitchen on same floor - ensure proper separation',
        fix: 'Verify bathroom does not open directly to kitchen'
      });
    }

    return warnings;
  }

  sumRoomAreas(rooms) {
    if (!rooms) return 0;
    return rooms.reduce((sum, room) => {
      const area = parseFloat(room.area);
      return sum + (isNaN(area) ? 0 : area);
    }, 0);
  }

  parseDimensions(dimStr) {
    const match = dimStr.match(/(\d+\.?\d*)m?\s*√ó\s*(\d+\.?\d*)m?/);
    if (match) {
      return [parseFloat(match[1]), parseFloat(match[2])];
    }
    return [5, 4];
  }

  generateAutoFixes(errors, warnings, dna) {
    // Generate corrected DNA with auto-fixes applied
    const fixes = [];

    errors.forEach(error => {
      if (error.type === 'DIMENSIONS' && error.severity === 'ERROR') {
        fixes.push({
          type: 'AUTO_FIX',
          original: error.message,
          action: error.fix
        });
      }
    });

    return fixes;
  }
}

export default new DesignValidator();
```

**Integration**:
```javascript
// In ArchitectAIEnhanced.js, BEFORE image generation
import designValidator from './services/designValidator';

// After DNA generation, before prompt generation (line ~1450)
console.log('üîç Validating Master DNA...');
const validation = designValidator.validate(masterDNA);

if (!validation.valid) {
  console.error('‚ùå DNA Validation Failed:');
  validation.errors.forEach(err => console.error(`   ${err.severity}: ${err.message}`));

  // Apply auto-fixes if available
  if (validation.autoFixes.length > 0) {
    console.log('üîß Applying auto-fixes...');
    masterDNA = this.applyAutoFixes(masterDNA, validation.autoFixes);
  }
}

if (validation.warnings.length > 0) {
  console.warn('‚ö†Ô∏è  DNA Validation Warnings:');
  validation.warnings.forEach(warn => console.warn(`   ${warn.message}`));
}
```

### 1.4 Add Diagnostic Script for Image Duplicate Detection

```javascript
// diagnose-image-duplicates.js (NEW FILE)
const fs = require('fs');
const path = require('path');

console.log('üîç Diagnosing Image Duplicate Issues\n');

// Check togetherAIService.js seed offsets
const servicePath = path.join(__dirname, 'src/services/togetherAIService.js');
const serviceContent = fs.readFileSync(servicePath, 'utf-8');

// Extract seed offsets
const seedOffsetsMatch = serviceContent.match(/const seedOffsets = \{([^}]+)\}/s);
if (seedOffsetsMatch) {
  console.log('‚úÖ Found seed offsets:');
  const offsets = seedOffsetsMatch[1];
  console.log(offsets);

  // Check if axonometric and perspective have different offsets
  const axonMatch = offsets.match(/axonometric_3d:\s*(\d+)/);
  const perspMatch = offsets.match(/perspective_3d:\s*(\d+)/);

  if (axonMatch && perspMatch) {
    const axonOffset = parseInt(axonMatch[1]);
    const perspOffset = parseInt(perspMatch[1]);

    if (axonOffset === perspOffset) {
      console.error('‚ùå ISSUE: Axonometric and Perspective have SAME seed offset!');
      console.error(`   Both use offset ${axonOffset}`);
      console.error('   FIX: Change perspective_3d to different offset (e.g., ${axonOffset + 1})');
    } else {
      console.log(`‚úÖ Seed offsets are different: axon=${axonOffset}, persp=${perspOffset}`);
    }
  }
}

// Check if prompts are properly differentiated
console.log('\nüîç Checking prompt generation...');
const promptGenPath = path.join(__dirname, 'src/services/dnaPromptGenerator.js');
if (fs.existsSync(promptGenPath)) {
  const promptContent = fs.readFileSync(promptGenPath, 'utf-8');

  const hasAxonMethod = promptContent.includes('generateAxonometricPrompt');
  const hasPerspMethod = promptContent.includes('generatePerspectivePrompt');

  if (hasAxonMethod && hasPerspMethod) {
    console.log('‚úÖ Both generateAxonometricPrompt and generatePerspectivePrompt exist');
  } else {
    console.error('‚ùå ISSUE: Missing prompt generation methods');
    if (!hasAxonMethod) console.error('   Missing: generateAxonometricPrompt');
    if (!hasPerspMethod) console.error('   Missing: generatePerspectivePrompt');
  }
}

// Check ArchitectAIEnhanced.js for state management
console.log('\nüîç Checking state management...');
const mainPath = path.join(__dirname, 'src/ArchitectAIEnhanced.js');
if (fs.existsSync(mainPath)) {
  const mainContent = fs.readFileSync(mainPath, 'utf-8');

  // Check if separate variables exist
  const hasAxonVar = mainContent.includes('axonometricUrl');
  const hasPerspVar = mainContent.includes('perspectiveUrl');

  if (hasAxonVar && hasPerspVar) {
    console.log('‚úÖ Separate state variables exist for axonometric and perspective');

    // Check if they're used in different img tags
    const imgTags = mainContent.match(/<img[^>]+src=\{[^}]+\}[^>]*>/g) || [];
    const axonImg = imgTags.filter(tag => tag.includes('axonometric'));
    const perspImg = imgTags.filter(tag => tag.includes('perspective'));

    console.log(`   Found ${axonImg.length} axonometric img tags`);
    console.log(`   Found ${perspImg.length} perspective img tags`);
  } else {
    console.warn('‚ö†Ô∏è  Could not find separate axonometric/perspective variables');
  }
}

console.log('\n========== RECOMMENDATIONS ==========\n');
console.log('1. Add logging in ArchitectAIEnhanced.js after image generation:');
console.log('   console.log("Axonometric URL:", axonometricUrl);');
console.log('   console.log("Perspective URL:", perspectiveUrl);');
console.log('   if (axonometricUrl === perspectiveUrl) {');
console.log('     console.error("BUG: Same URL for both views!");');
console.log('   }');
console.log('');
console.log('2. Ensure unique React keys on img elements:');
console.log('   <img key={`axon-${timestamp}`} src={axonometricUrl} />');
console.log('   <img key={`persp-${timestamp}`} src={perspectiveUrl} />');
console.log('');
console.log('3. Generate a test design and check browser console for duplicate URLs');
```

**Phase 1 Deliverables**:
- ‚úÖ Fixed duplicate image bug
- ‚úÖ Persistent design.json in localStorage
- ‚úÖ Pre-generation validation (topology, dimensions, materials)
- ‚úÖ Diagnostic tooling

**Expected Impact**: 98% ‚Üí 98.5% consistency (minor improvement, but better debugging and state management)

---

## Phase 2: Hybrid Approach (1 Week)
### Add geometry layer while keeping AI generation

### 2.1 Add Procedural Floor Plan Generator

**Goal**: Generate deterministic 2D floor plans from DNA, then optionally AI-stylize

```javascript
// src/services/proceduralFloorPlanGenerator.js (NEW FILE)
class ProceduralFloorPlanGenerator {
  generateFloorPlan(designState, floor) {
    const { dimensions, rooms, doors, windows, dna } = designState;

    // 1. CREATE SVG CANVAS
    const svg = this.createSVG(dimensions.length * 100, dimensions.width * 100);

    // 2. GENERATE ROOM LAYOUT
    const layout = this.generateRoomLayout(rooms.filter(r => r.level === floor), dimensions);

    // 3. DRAW WALLS
    layout.forEach(room => {
      this.drawRoom(svg, room, dna.materials.wallThickness);
    });

    // 4. ADD DOORS
    doors.filter(d => d.level === floor).forEach(door => {
      this.drawDoor(svg, door);
    });

    // 5. ADD WINDOWS
    windows.filter(w => w.level === floor).forEach(window => {
      this.drawWindow(svg, window);
    });

    // 6. ADD DIMENSIONS & ANNOTATIONS
    this.addDimensions(svg, layout);
    this.addRoomLabels(svg, layout);

    // 7. EXPORT
    const floorPlanSVG = svg.toString();
    const floorPlanPNG = this.svgToPng(floorPlanSVG);

    return {
      svg: floorPlanSVG,
      png: floorPlanPNG,
      isDeterministic: true
    };
  }

  generateRoomLayout(rooms, buildingDimensions) {
    // Spatial layout algorithm
    // Option A: Grid-based packing
    // Option B: Use AI (Qwen) to solve spatial constraints
    // Option C: Rule-based heuristics

    // For Phase 2, use simple rule-based
    const layout = [];
    let currentX = 0;
    let currentY = 0;

    rooms.forEach(room => {
      const [length, width] = this.parseDimensions(room.dimensions);

      layout.push({
        id: room.id,
        name: room.name,
        poly: [
          [currentX, currentY],
          [currentX + length, currentY],
          [currentX + length, currentY + width],
          [currentX, currentY + width]
        ]
      });

      // Simple horizontal stacking (naive)
      currentX += length;
      if (currentX > buildingDimensions.length) {
        currentX = 0;
        currentY += width;
      }
    });

    return layout;
  }

  createSVG(width, height) {
    // Return SVG builder object
    return {
      elements: [],
      width,
      height,
      addRect: function(x, y, w, h, style) {
        this.elements.push(`<rect x="${x}" y="${y}" width="${w}" height="${h}" style="${style}" />`);
      },
      addLine: function(x1, y1, x2, y2, style) {
        this.elements.push(`<line x1="${x1}" y1="${y1}" x2="${x2}" y2="${y2}" style="${style}" />`);
      },
      addText: function(x, y, text, style) {
        this.elements.push(`<text x="${x}" y="${y}" style="${style}">${text}</text>`);
      },
      toString: function() {
        return `<svg width="${this.width}" height="${this.height}" xmlns="http://www.w3.org/2000/svg">\n  ${this.elements.join('\n  ')}\n</svg>`;
      }
    };
  }

  drawRoom(svg, room, wallThickness) {
    const style = 'stroke:black;stroke-width:2;fill:none';
    room.poly.forEach((point, i) => {
      const nextPoint = room.poly[(i + 1) % room.poly.length];
      svg.addLine(point[0], point[1], nextPoint[0], nextPoint[1], style);
    });
  }

  svgToPng(svgString) {
    // Use canvas API to convert SVG to PNG
    // In browser: document.createElement('canvas')
    // In Node: use 'canvas' package
    return 'data:image/png;base64,...'; // placeholder
  }
}

export default new ProceduralFloorPlanGenerator();
```

### 2.2 Add ControlNet Pipeline for AI Stylization

**Use procedural floor plans as ControlNet input** to guide FLUX

```javascript
// src/services/controlNetStyler.js (NEW FILE)
class ControlNetStyler {
  async stylizeFloorPlan(proceduralPNG, designState) {
    // 1. Extract edge map from procedural plan
    const edgeMap = await this.extractEdges(proceduralPNG);

    // 2. Generate style guidance from DNA
    const stylePrompt = this.generateStylePrompt(designState.dna);

    // 3. Call FLUX with ControlNet
    const stylizedImage = await this.callFluxWithControlNet({
      prompt: stylePrompt,
      controlnetImage: edgeMap,
      controlnetType: 'canny', // or 'depth', 'normal', 'lineart'
      controlnetStrength: 0.75, // Keep geometry, adjust style
      seed: designState.dna.seed,
      width: 1024,
      height: 768
    });

    return {
      procedural: proceduralPNG,
      stylized: stylizedImage,
      preservesGeometry: true
    };
  }

  async extractEdges(imagePNG) {
    // Run Canny edge detection
    // Can use canvas API or call Python script
    return 'data:image/png;base64,...'; // edge map
  }

  generateStylePrompt(dna) {
    return `Architectural floor plan, ${dna.style} style, clean CAD drawing, black lines on white background, technical blueprint aesthetic`;
  }

  async callFluxWithControlNet(params) {
    // Call Together.ai FLUX.1 with ControlNet
    // NOTE: Check if Together.ai supports ControlNet or use Replicate
    const response = await fetch('/api/together/image', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: 'black-forest-labs/FLUX.1-dev-Controlnet',
        prompt: params.prompt,
        controlnet_image: params.controlnetImage,
        controlnet_type: params.controlnetType,
        controlnet_strength: params.controlnetStrength,
        seed: params.seed,
        width: params.width,
        height: params.height
      })
    });

    return response.json();
  }
}

export default new ControlNetStyler();
```

### 2.3 Hybrid Generation Pipeline

**Combine procedural + AI approaches**

```javascript
// In ArchitectAIEnhanced.js or new orchestration service
async function generateHybridDesign(designState) {
  console.log('üîß Using Hybrid Generation (Procedural + AI)');

  const results = {};

  // 1. PROCEDURAL FLOOR PLANS (100% deterministic)
  console.log('üìê Step 1: Generating procedural floor plans...');
  results.floorPlans = {
    ground: proceduralFloorPlanGenerator.generateFloorPlan(designState, 0),
    upper: proceduralFloorPlanGenerator.generateFloorPlan(designState, 1)
  };

  // 2. OPTIONAL AI STYLIZATION
  if (userPreference.aiStylization) {
    console.log('üé® Step 2: AI stylizing floor plans...');
    results.floorPlans.ground.stylized = await controlNetStyler.stylizeFloorPlan(
      results.floorPlans.ground.png,
      designState
    );
    results.floorPlans.upper.stylized = await controlNetStyler.stylizeFloorPlan(
      results.floorPlans.upper.png,
      designState
    );
  }

  // 3. AI-GENERATED 3D VIEWS (existing workflow)
  console.log('üèóÔ∏è Step 3: Generating 3D views with FLUX...');
  results.exterior3D = await togetherAIService.generate3DViews(designState.dna);

  return results;
}
```

**Phase 2 Deliverables**:
- ‚úÖ Procedural 2D floor plan generator (SVG ‚Üí PNG)
- ‚úÖ ControlNet stylization pipeline
- ‚úÖ Hybrid workflow (deterministic 2D + AI 3D)
- ‚úÖ User choice: pure procedural vs AI-stylized

**Expected Impact**: 98% ‚Üí 99.5% consistency (2D plans now 100% accurate)

---

## Phase 3: Full Geometry Engine (2-4 Weeks)
### Complete transition to geometry-first architecture

### 3.1 3D Geometry Generation

```javascript
// src/services/geometryEngine.js (NEW FILE)
// Uses Three.js or similar for 3D geometry
class GeometryEngine {
  buildModel(designState) {
    const { dimensions, rooms, doors, windows, dna } = designState;

    // 1. Create 3D scene
    const scene = new THREE.Scene();

    // 2. Extrude floor plans to 3D volumes
    rooms.forEach(room => {
      const shape = this.polyToShape(room.poly);
      const extrudeSettings = {
        depth: designState.levels[room.level].height_mm / 1000,
        bevelEnabled: false
      };
      const geometry = new THREE.ExtrudeGeometry(shape, extrudeSettings);
      const material = new THREE.MeshStandardMaterial({ color: 0xcccccc });
      const mesh = new THREE.Mesh(geometry, material);
      mesh.position.z = designState.levels[room.level].z / 1000;
      scene.add(mesh);
    });

    // 3. Add roof geometry
    const roof = this.generateRoof(dna.roof, dimensions);
    scene.add(roof);

    // 4. Boolean operations for doors/windows
    doors.forEach(door => this.cutOpening(scene, door, 'door'));
    windows.forEach(window => this.cutOpening(scene, window, 'window'));

    return {
      scene,
      geometry: this.exportGeometry(scene),
      isDeterministic: true
    };
  }

  renderView(scene, camera, renderer) {
    // Render specific view from 3D model
    renderer.render(scene, camera);
    return renderer.domElement.toDataURL();
  }

  exportGeometry(scene) {
    // Export as glTF, OBJ, or IFC
    const exporter = new GLTFExporter();
    return exporter.parse(scene);
  }
}
```

### 3.2 View Rendering from Geometry

```javascript
// All views derived from same 3D model
const cameras = {
  groundPlan: { type: 'ortho', position: [0, 0, 50], lookAt: [0, 0, 0], up: [0, 1, 0] },
  northElev: { type: 'ortho', position: [0, -50, 3], lookAt: [0, 0, 3], up: [0, 0, 1] },
  axonometric: { type: 'ortho', position: [30, 30, 20], lookAt: [0, 0, 0], up: [0, 0, 1] },
  perspective: { type: 'persp', position: [25, -25, 5], lookAt: [0, 0, 3], fov: 60 }
};

// Render all views
Object.entries(cameras).forEach(([viewName, camConfig]) => {
  const camera = this.setupCamera(camConfig);
  const render = geometryEngine.renderView(scene, camera, renderer);
  results[viewName] = {
    render,
    isDeterministic: true,
    geometry: 'consistent' // All from same model!
  };
});
```

### 3.3 AI Stylization Layer (Optional)

```javascript
// Use geometry renders as ControlNet for photorealism
async function addPhotorealismPass(geometryRenders, designState) {
  const stylized = {};

  for (const [viewName, render] of Object.entries(geometryRenders)) {
    // Use depth + normal maps as ControlNet inputs
    const depthMap = await extractDepthMap(render);
    const normalMap = await extractNormalMap(render);

    stylized[viewName] = await togetherAIService.generate({
      prompt: `Photorealistic ${viewName}, ${designState.dna.materials.exterior.primary}, ${designState.dna.roof.material} roof`,
      controlnets: [
        { type: 'depth', image: depthMap, strength: 0.9 },
        { type: 'normal', image: normalMap, strength: 0.8 }
      ],
      seed: designState.dna.seed,
      baseImage: render // Optional: use geometry render as base
    });
  }

  return stylized;
}
```

**Phase 3 Deliverables**:
- ‚úÖ Full 3D geometry engine (Three.js or similar)
- ‚úÖ All 13 views rendered from same model
- ‚úÖ Export as IFC, glTF, OBJ for BIM tools
- ‚úÖ Optional AI stylization layer
- ‚úÖ 100% geometric consistency guaranteed

**Expected Impact**: 99.5% ‚Üí 99.9%+ consistency (all views from single 3D model)

---

## Implementation Priority Matrix

| Task | Impact | Effort | Priority | Timeline |
|------|--------|--------|----------|----------|
| Fix axon/persp bug | Low | 1 day | HIGH | Immediate |
| Add design.json persistence | Medium | 1 day | HIGH | Immediate |
| Add validation layer | High | 2 days | HIGH | Week 1 |
| Procedural floor plans | Very High | 3 days | HIGH | Week 1-2 |
| ControlNet stylization | Medium | 3 days | MEDIUM | Week 2 |
| Hybrid workflow integration | High | 2 days | HIGH | Week 2 |
| Full 3D geometry engine | Very High | 2 weeks | LOW | Month 2 |
| AI stylization layer | Low | 1 week | LOW | Month 2 |

---

## Decision Points

### Question 1: Full Refactor or Incremental?

**Option A: Incremental (Recommended)**
- Implement Phase 1 ‚Üí Phase 2 ‚Üí Phase 3
- Preserve current 98% system while building new one
- Lower risk, continuous delivery

**Option B: Big Bang Refactor**
- Rebuild entire system with geometry-first approach
- Higher risk, but cleaner architecture
- 2-4 week development freeze

**Recommendation**: Start with Phase 1 (immediate fixes), then decide based on results.

### Question 2: Which 2D Generation Approach?

**Option A: Pure Procedural (Your Proposal)**
- SVG-based floor plan generation
- 100% deterministic, mathematically correct
- Requires spatial layout solver

**Option B: Hybrid (Recommended for Phase 2)**
- Procedural geometry + AI stylization
- Combines accuracy with aesthetic appeal
- Lower development effort

**Option C: AI-Only (Current System)**
- DNA prompts ‚Üí FLUX
- 98% consistency ceiling
- Already working

**Recommendation**: Phase 2 Hybrid ‚Üí Phase 3 Pure Procedural

### Question 3: 3D Generation Strategy?

**Option A: Geometry-First (Your Proposal)**
- Three.js 3D model ‚Üí renders
- 100% consistent views
- Requires geometry engine development

**Option B: ControlNet-Guided AI (Hybrid)**
- Geometry renders as ControlNet input
- AI adds photorealism
- Faster to implement

**Option C: AI-Only (Current System)**
- FLUX generates all 3D views
- No geometric guarantees
- Already working

**Recommendation**: Phase 2 ControlNet Hybrid ‚Üí Phase 3 Geometry-First

---

## Diagnostic Checklist for Current Issues

Before implementing any changes, run these diagnostics:

### Check 1: Are Axonometric and Perspective Actually Different?

```bash
# Run this in browser console after generation
console.log('Axonometric:', result.visualizations.views.axonometric);
console.log('Perspective:', result.visualizations.views.perspective);

// Check if URLs are identical
const axonUrl = result.visualizations.views.axonometric?.images?.[0];
const perspUrl = result.visualizations.views.perspective?.images?.[0];
if (axonUrl === perspUrl) {
  console.error('BUG CONFIRMED: Same URL');
} else {
  console.log('URLs are different - not a generation bug, likely a display bug');
}
```

### Check 2: What is the Current Consistency Score?

Run test generation and manually compare:
- Do ground and upper floor plans have matching room positions?
- Do all 4 elevations show same material colors (#8B4513 brick, #654321 roof)?
- Do 3D exterior views match floor plan dimensions?
- Does main entrance appear in same location (north facade) in all views?

### Check 3: Is DNA Actually Being Used?

```bash
# Check console logs during generation
# Should see:
‚úÖ Master Design DNA generated successfully
   Dimensions: 15m √ó 10m
   Materials: Red brick (#8B4513)

# If you see this instead:
‚ö†Ô∏è Falling back to legacy generation

# Then DNA is NOT being used - check why
```

---

## Quick Start: Implementing Phase 1

**Day 1 Morning**: Fix axonometric/perspective bug
1. Edit `src/ArchitectAIEnhanced.js` lines 227-229
2. Add logging to detect duplicate URLs
3. Add unique React keys to images
4. Test generation, check console

**Day 1 Afternoon**: Add design.json persistence
1. Create `src/services/designStateManager.js`
2. Integrate in ArchitectAIEnhanced.js after DNA generation
3. Test: verify localStorage contains design_proj_XXX

**Day 2**: Add validation layer
1. Create `src/services/designValidator.js`
2. Integrate before image generation
3. Test: try generating design with invalid dimensions

**Day 3**: Testing and documentation
1. Generate 5 test designs
2. Measure consistency improvements
3. Document results

---

## Expected Outcomes by Phase

**After Phase 1 (Week 1)**:
- Bug fixes and better debugging
- Persistent design state
- Validation catches impossible geometries
- Consistency: 98% ‚Üí 98.5%

**After Phase 2 (Week 2-3)**:
- 2D floor plans are 100% accurate (procedural)
- 3D views maintain 98% consistency (AI)
- User can choose deterministic vs stylized
- Consistency: 98.5% ‚Üí 99.5% (floor plans perfect)

**After Phase 3 (Month 2)**:
- All views from single 3D model (100% geometric consistency)
- AI optional (only for photorealism)
- Full BIM export capability
- Consistency: 99.5% ‚Üí 99.9%+

---

## Cost-Benefit Analysis

### Current System (DNA + FLUX):
- **Cost per design**: $0.15-0.23
- **Consistency**: 98%
- **Speed**: 3 minutes (13 views)
- **Pros**: Working now, relatively fast
- **Cons**: AI variability, can't guarantee 100%

### Proposed System (Geometry + Optional AI):
- **Cost per design**: $0.05-0.10 (fewer AI calls)
- **Consistency**: 99.9%+
- **Speed**: 2 minutes (faster geometry renders)
- **Pros**: Deterministic, BIM-ready, mathematically correct
- **Cons**: Requires geometry engine development, less "artistic"

---

## Conclusion & Recommendations

### Summary of Analysis

Your proposal is **architecturally sound** and represents best practices in computational design. The transition from AI-driven to geometry-driven generation is the right long-term direction for achieving deterministic consistency.

**Key Finding**: The current DNA system is already 80% of the way to your proposed design.json! The main gap is the lack of a geometry engine to enforce geometric consistency.

###Recommended Path Forward

**Immediate (This Week)**: Implement Phase 1
1. Fix axonometric/perspective bug
2. Add design.json persistence
3. Add validation layer
4. Run diagnostics on current system

**Short-Term (Next 2-3 Weeks)**: Implement Phase 2 Hybrid
1. Build procedural floor plan generator
2. Add ControlNet stylization option
3. Give users choice: deterministic vs AI-stylized
4. Measure consistency improvements

**Long-Term (Month 2+)**: Consider Phase 3
1. Evaluate Phase 2 results
2. If 99.5% consistency sufficient ‚Üí stop at Phase 2
3. If need 99.9%+ ‚Üí build full geometry engine
4. Add IFC/BIM export for professional workflows

### Critical Decision Points

**Before starting Phase 2 or 3, answer these questions**:

1. **What consistency level is actually required?**
   - If 98-99% is acceptable ‚Üí Current system works
   - If 99.5% needed ‚Üí Phase 2 Hybrid sufficient
   - If 99.9%+ required ‚Üí Full Phase 3 refactor

2. **What is the primary use case?**
   - Conceptual design ‚Üí AI stylization valuable
   - Technical documentation ‚Üí Pure procedural better
   - BIM integration ‚Üí Geometry engine required

3. **What are the resource constraints?**
   - 1 week available ‚Üí Phase 1 only
   - 2-3 weeks ‚Üí Phase 1 + Phase 2
   - 1-2 months ‚Üí Full Phase 3

### Questions for You

To prioritize implementation, please clarify:

1. **Urgency**: Do you need immediate fixes (axon/persp bug) or are you planning a full refactor?

2. **Scope**: Are you targeting 99.5% consistency (Phase 2 Hybrid) or 99.9%+ (Phase 3 Geometry)?

3. **Resources**: How much development time can you allocate?
   - Phase 1: 1 week (1 developer)
   - Phase 2: 2-3 weeks (1-2 developers)
   - Phase 3: 1-2 months (2-3 developers)

4. **Use Case**: What's more important?
   - Speed and aesthetics (favor AI)
   - Accuracy and BIM compatibility (favor geometry)
   - Balance of both (hybrid approach)

5. **Technical Constraints**: Do you have experience with:
   - Three.js or other 3D libraries?
   - Computational geometry algorithms?
   - ControlNet and image preprocessing?

---

## Next Steps

Based on your answers, I can help with:

1. **Immediate Implementation**:
   - Fix axonometric/perspective bug right now
   - Add logging and diagnostics
   - Implement Phase 1 validation layer

2. **Architecture Planning**:
   - Design procedural floor plan generator
   - Specify ControlNet integration points
   - Plan geometry engine architecture

3. **Proof of Concept**:
   - Build minimal procedural floor plan generator
   - Test hybrid workflow with one view
   - Measure consistency improvements

4. **Full Implementation**:
   - Implement chosen phase end-to-end
   - Test with real projects
   - Deploy to production

**What would you like to prioritize first?**

---

## Appendix: Your Original Proposal Alignment

### How This Roadmap Addresses Your 9 Points

1. **‚úÖ Single source of truth (design.json)**: Phase 1.2 - designStateManager.js
2. **‚úÖ Procedural geometry first**: Phase 2.1 & 3.1 - floor plan & 3D generators
3. **‚úÖ Design rules validation**: Phase 1.3 - designValidator.js
4. **‚úÖ Fix "same image twice" bug**: Phase 1.1 - diagnostic & fix
5. **‚úÖ ControlNet/IP-Adapter**: Phase 2.2 & 3.3 - stylization layers
6. **‚úÖ Project DNA as tokens**: Already implemented - enhance in Phase 1.2
7. **‚úÖ Technical drawings from model**: Phase 3.2 - geometry-based renders
8. **‚úÖ Code restructuring**: Incremental refactor across all phases
9. **‚úÖ Diagnostic checklist**: Phase 1.4 & diagnostic section above

**All 9 of your recommendations are incorporated in this roadmap.**

---

## Contact & Support

**Documentation**:
- This roadmap: `CONSISTENCY_ENHANCEMENT_ROADMAP.md`
- Current DNA system: `DNA_SYSTEM_ARCHITECTURE.md`
- Consistency fixes: `CONSISTENCY_SYSTEM_COMPLETE.md`

**Diagnostic Tools**:
- Environment check: `npm run check:env`
- Consistency diagnostic: `node diagnose-consistency.js`
- Image duplicate check: `node diagnose-image-duplicates.js` (create from Phase 1.4)

**For Implementation Assistance**:
- Phase 1 fixes: Can be implemented immediately
- Phase 2 architecture: Requires design decisions (see questions above)
- Phase 3 planning: Requires full technical spec and timeline

---

**Ready to proceed with Phase 1 implementation? Let me know which tasks to prioritize.**
